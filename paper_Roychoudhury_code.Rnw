\documentclass[a4paper, 11pt]{article}
\usepackage[margin=0.9in]{geometry}
\usepackage{amsmath, amssymb} % math
\usepackage[round]{natbib} % bibliography
\usepackage[dvipsnames,table]{xcolor} % colors
\usepackage[onehalfspacing]{setspace} % more space
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}


%% margins
% \usepackage{geometry}
% \geometry{
%   a4paper,
%   total={170mm,257mm},
%   left=20mm,
%   right=20mm,
%   top=30mm,
%   bottom=30mm,
% }

\newcommand\mail{yangmi@ethz.ch}
\title{\textbf{Code exploration}}
\author{Minghan Yang}
\date{\today}

\usepackage{hyperref}
\hypersetup{
  bookmarksopen=true, 
  breaklinks=true,
  pdfsubject={},
  pdfkeywords={},
  colorlinks=true,
  linkcolor=black,
  anchorcolor=black,
  citecolor=blue,
  urlcolor=blue,
}

<< "main-setup", echo = FALSE>>=
## setting knitr options (see https://yihui.org/knitr/ for all the options)
library(knitr)
opts_chunk$set(fig.height = 4,
               echo = FALSE,
               warning = FALSE,
               message = FALSE
               )

# Load necessary libraries
library(ggplot2)
library(cowplot)
library(gridExtra)
library(grid)
library(kableExtra)
@

\begin{document}
\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
%\SweaveOpts{concordance=TRUE}
\maketitle


\begin{abstract}
Related to paper ``Beyond p-values: A phase II dual-criterion design with statistical significance and clinical relevance'' \citet{Roychoudhury2018}.
\end{abstract}

\section{Introduction}
Proof-of-concept (POC) in Phase II trails is important in investigating the efficacy of an experimental drug. It will influence the decision of whether continuing or not continuing the development of this drug. \\
Dual-criterion design in frequentist and Bayesian applications are discussed. \\
Three generic phase II designs are reviewed:
\begin{enumerate}
\item Standard design\\
For comparative treatment and control trails, it puts forward criteria expressed as error rates: Type I error control and Power (correctly reject $H_0$ when it is false).\\
Control type I error and maximize power.\\
Type I error: $\mathbb{P}(\text{reject } H_0 | H_0 \text{ is true})=\alpha$\\
Type II error: $\mathbb{P}(\text{not reject } H_0 | H_0 \text{ is false})=\beta$.\\
Limitation: statistical significant only guarantees evidence to reject ``No effect", but is not sufficient for clinical perspective. Also, it always result in success or failure according to statistical significance.\\
Increasing the sample size increases the power for effects better than null.
\item Dual-criterion design\\
Considers both the statistical significance and the effect estimate. \\
Required inputs: type I error control (null hypothesis and type I error $\alpha$) and a decision value (DV). The DV is same as the target difference in Fisch's paper. It is the minimal effect estimate needed for trail success (if higher than this value with moderate confidence, then GO).\\
By considering both, we have both statistical significance and guarantees a sufficiently large effect estimate.\\
The dual-criterion is more demanding, the resulting power of study is less compared to standard design.\\
Power is only increased for values superior to the DV since inferior values are clinically irrelevant.\\
Decisions for dual-criterion design:
\begin{center}
<<plot1, echo=FALSE, fig=TRUE>>=
# Updated data based on visual inspection of the plot
data <- data.frame(
  group = factor(c("(1) NO-GO", "(2) GO", "(3)Inconclusive", "(4)Inconclusive"), 
                 levels = c("(4)Inconclusive", "(3)Inconclusive", "(2) GO","(1) NO-GO")),
  hazard_ratio = c(0.85, 0.6, 0.8, 0.65),     # Hazard ratio estimates for each case
  lower_ci = c(0.55, 0.25, 0.65, 0.25),         # Lower bounds of CIs
  upper_ci = c(1.15, 0.95, 0.95, 1.05)          # Upper bounds of CIs
)

# Plot setup
ggplot(data, aes(x = hazard_ratio, y = group)) +
  geom_point(shape = 18, size = 3) +   # Diamond marker for hazard ratios
  geom_errorbarh(aes(xmin = lower_ci, xmax = upper_ci), height = 0.2) + # Error bars for CIs
  geom_vline(xintercept = 1, linetype = "dashed", color = "grey50") +  # Line for HR=1
  geom_vline(xintercept = 0.7, linetype = "dashed", color = "grey50") + # Line for DV (assumed to be 0.7)
  annotate("text", x = 0.7, y = 0, label = "DV", vjust = -1, color = "red",fontface = "bold", size = 5) + # Label for DV
  annotate("text", x = 1.02, y = 0, label = "1", vjust = -1, color = "red",fontface = "bold", size = 5) +   # Label for HR=1
  labs(x = "Hazard ratio", y = "") +
  scale_x_continuous(breaks = NULL, limits = c(0.2, 1.5)) +
  theme_minimal(base_size = 14) +
  theme(panel.grid.minor = element_blank()) +
  annotate("text", x = 0.4, y = 4.5, label = "Treatment better", hjust = 0, color = "grey20") +
  annotate("text", x = 1.4, y = 4.5, label = "Treatment worse", hjust = 1, color = "grey20")
@
\end{center}

\item Precision design\\
Doesn't rely on error rates. When null hypothesis or other benchmark values cannot be determined, this can be an option. It requires sufficiently precise effect estimate.\\
Precision=$\frac{\text{TP}}{\text{TP}+\text{FP}}$. High Precision means that when the model predicts a positive outcome, it is very likely to be correct.
\end{enumerate}

\subsection{Hazard ratio and log hazard ratio}
Consider hazard function in survival analysis, it describes the risk of failing. We consider hazard ratio between experimental drug and control as the outcome of interest. Hazard ratio(HR) less than 1 means the drug is better than the control. We want to have smaller hazard and hazard ratio, so that the drug is more effective. \\
In the paper, the log hazard ratio ($\log$ HR) is used instead of the hazard ratio. This could be because of the following reasons.\\
Firstly, according to the Central Limit Theorem, as the sample size increases, the distribution of the estimator (the log HR) approaches a normal distribution. Also, the HR itself is a ratio of hazard rates and is positively skewed, meaning it doesnâ€™t naturally fit a normal distribution. Taking the logarithm of the HR stabilizes its variance, transforming the skewed HR distribution into one that is more symmetric and closer to normal. Moreover, this approximate normality of $\log$ HR is useful in sample size calculation, which is discussed in the next session.

\subsection{Sample size}
Given the significance level $\alpha$, the null value (NV), and the decision value (DV), we can calculate the minimum sample size (for normally distributed data):
\begin{equation*}
n_{\text{min}}=\frac{\sigma^2 \times z_{\alpha}^2}{(\text{NV}-\text{DV})^2}
\end{equation*}
where $\sigma$ is the outcome standard deviation, and takes the value 2 under equal randomization for the standard normal approximation to time-to-event data. 
%In the log scale, if we say the value is within $\pm 2$ about the truth, then this translates into a range of approximately $\pm 100 \time 2 \%$ about the truth on the original scale. 
The $z_{\alpha}$ is the $100(1-\alpha)\%$ quantile of the standard normal distribution. The $n_{\text{min}}$ gives the minimum sample size that implies statistical significance if the effect estimate equals the DV. This value is calculated under the situation that both criterion are just satisfied. As illustrated in the below graph, when the effect estimate $\theta=$ DV, and the lower bound of the confidence interval just touches the NV so that statistical significance is reached, we have the minimum sample size. Notice that when sample size equals the minimum sample size, the width of the confidence interval $z_{\alpha}\sqrt{\frac{\sigma^2}{n_{\text{min}}}}$ equals NV-DV, so there will be no ``Inconclusive'' decisions. When the sample size is larger, the confidence interval becomes narrower, then an ``Inconclusive'' decision will occur.
\begin{center}
<< Minimum sample size, echo=FALSE, results=hide, fig=TRUE>>=
# Define x-axis range and labels
x <- c(5.5,10)
par(mar = c(4, 4, 2, 2) + 0.1)
# Set up an empty plot with x-axis labeled points
plot(1:6, type = "n", xaxt = "n", ylim = c(0.2, 0.5), yaxt = "n", ylab = "Y", xlab = "Hazard Ratio", xlim=c(-2,12))
axis(1, at = x, labels = c("DV", "NV=1"))

# Add vertical slim strip constrained in y-axis between 0.6 and 0.63
rect(1, 0.3, 10, 0.32, col = rgb(0.5, 0.5, 1, alpha = 0.3), border = NA)

# Add vertical dashed lines at x = 1 and x = 5.5
abline(v = x, col = "black", lty = 2)
abline(v=1, col="black",lty=2)

library(latex2exp)
text(x=1.3, y=0.35, labels = TeX("$\\theta-z_{\\alpha} \\sqrt{\\frac{\\sigma^2}{n_{min}}}$"),cex = 1.5)
text(x=5.7, y=0.34, labels = TeX("$\\theta$"),cex=1.5)
text(x=9.7, y=0.35, labels = TeX("$\\theta+z_{\\alpha} \\sqrt{\\frac{\\sigma^2}{n_{min}}}$"),cex=1.5)
text(x=5, y=0.31, labels = "confidence interval", cex=1)
@ 
\end{center}

\subsection{Operating characteristics}
The operating characteristics are the type I error and power of the clinical trial design. \\
For dual-criterion designs, the power at the DV is approximately 50\%, so that if the true parameter equals the DV, there is roughly equal chance that the effect estimate lies on either side of the DV. Having 50\% at the DV does not mean the study is under-powered.

\subsection{Reproduce Figure 1} \label{1.4}
In Figure 1, the two plots illustrate the operating characteristics of dual-criterion designs with 309 and 420 events. The number 309 is the minimum sample size calcualted under the example conditions $\sigma=2$, $\alpha=2.5\%$, $\log$ hazard ratios NV = $\log(1)$, DV = $\log(0.8)$.
\begin{equation*}
n_{\text{min}}=\frac{2^2 \times z_{0.025}^2}{(\log{1}-\log{0.8})^2} = 308.594 \approx 309
\end{equation*}
The probability of making a ``GO'' decision is the probability of the estimate smaller than the DV (i.e. clinical relevance) while the NV is outside the confidence interval (i.e. statistical significance). The ``NO-GO'' decision is made when the estimate is larger than the DV, and the NV is inside the confidence interval. The ``Inconclusive'' decisions are made if neither ``GO'' nor ``NO-GO'' is satisfied. The probability of ``Inconclusive'' decision is hence 1 minus the probability of ``GO'' and ``NO-GO'' decisions. \\
Below code presents the process of obtaining Figure 1 in the paper \citet{Roychoudhury2018}. An extra notice here is the calculation of the cutting value for statistical significance when the sample size is larger than $n_{\text{min}}$. As we mentioned earlier, when $n > n_{\text{min}}$, the confidence interval is shorter, so the cutting value \texttt{cut.ssig} of the ``NO-GO'' decision should be a value between DV and NV such that $z_{\alpha}\sqrt{\frac{\sigma^2}{n}} = \text{NV} - \texttt{cut.ssig}$. So in this case, the $\texttt{cut.ssig} = \log(1) - z_{\alpha}\sqrt{\frac{\sigma^2}{n}}$.
\begin{center}
<<Figure 1, fig=TRUE, results=hide>>=
# Sequence of true hazard ratios in log scale
t.d <- log(seq(0.5, 1, 0.01))

# Left panel (n = 309)
n1 <- 309
sd1 <- sqrt((2^2) / n1) # standard deviation
cut.ssig1 <- log(0.8) # cutting point for statistical significance
cut.crel1 <- log(0.8) # cutting point for clinical relevance
pp.go1 <- pnorm(cut.crel1, t.d, sd1) # probability of GO decision
pp.ngo1 <- 1 - pnorm(cut.ssig1, t.d, sd1) # probability of NO-GO decision
pp.intd1 <- 1 - pp.go1 - pp.ngo1 # probability of inconclusive decision
df1 <- data.frame(HazardRatio = exp(t.d), GO = pp.go1, 
                  NOGO = pp.ngo1, Inconclusive = pp.intd1)

# Right panel (n = 420)
n2 <- 420
sd2 <- sqrt((2^2) / n2) # standard deviation
cut.ssig2 <- log(1) - qnorm(0.975) * sqrt(2^2 / 420) # statistical significance
cut.crel2 <- log(0.8) # clinical relevance
pp.go2 <- pnorm(cut.crel2, t.d, sd2) # probability of GO decision
pp.ngo2 <- 1 - pnorm(cut.ssig2, t.d, sd2) # probability of NO-GO decision
pp.intd2 <- 1 - pp.go2 - pp.ngo2 # probability of inconclusive decision
df2 <- data.frame(HazardRatio = exp(t.d), GO = pp.go2, 
                  NOGO = pp.ngo2, Inconclusive = pp.intd2)

# Define the line colors and types
line_colors <- c("GO" = "green3", "Inconclusive" = "darkgoldenrod2", "NO-GO" = "red3")
line_types <- c("GO" = "solid", "Inconclusive" = "dotted", "NO-GO" = "dashed")

# Plot for n = 309
p1 <- ggplot(df1, aes(x = HazardRatio)) +
  geom_line(aes(y = GO, color = "GO", linetype = "GO"),size=0.8) +
  geom_line(aes(y = NOGO, color = "NO-GO", linetype = "NO-GO"),size=0.8) +
  geom_line(aes(y = Inconclusive, color = "Inconclusive", 
                linetype = "Inconclusive"),size=1.5) +
  geom_vline(xintercept = c(0.8, 1.0), linetype = "dashed", color = "black") +
  labs(title = "Nevent = 309") +
  scale_color_manual(values = line_colors) +
  scale_linetype_manual(values = line_types) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank(),
    axis.title.y = element_blank(),  # Remove individual y-labels
    axis.title.x = element_blank()
  )

# Plot for n = 420
p2 <- ggplot(df2, aes(x = HazardRatio)) +
  geom_line(aes(y = GO, color = "GO", linetype = "GO"),size=0.8) +
  geom_line(aes(y = NOGO, color = "NO-GO", linetype = "NO-GO"),size=0.8) +
  geom_line(aes(y = Inconclusive, color = "Inconclusive", 
                linetype = "Inconclusive"),size=1.5) +
  geom_vline(xintercept = c(0.8, 1.0), linetype = "dashed", color = "black") +
  labs(title = "Nevent = 420") +
  scale_color_manual(values = line_colors) +
  scale_linetype_manual(values = line_types) +
  theme_minimal() +
  theme(
    legend.position = "none",
    plot.title = element_text(hjust = 0.5, face = "bold"),
    panel.grid.minor = element_blank(),
    axis.title.y = element_blank(),  # Remove individual y-labels
    axis.title.x = element_blank()
  )

# Combine the plots into a single figure without individual y-axis labels
combined_plots <- plot_grid(p1, p2, ncol = 2, align = 'hv', rel_widths = c(1, 1))

# Extract and create a shared legend
legend <- get_legend(
  p1 + theme(legend.position = "right") +
    guides(color = guide_legend(title = "Decision", nrow = 1), 
           linetype = guide_legend(title = "Decision", nrow = 1)))

# Add a shared y-axis label using grid.arrange
final_plot <- grid.arrange(
  arrangeGrob(combined_plots, 
              left = textGrob("Probability", rot = 90, vjust = 1.2),
              bottom = textGrob("Hazard Ratio",just = "centre")),
  legend = legend,
  ncol = 1,
  heights = c(10, 1) 
)

# Print the final plot
print(final_plot)
@
\end{center}

\section{Example 1: A randomized PoC design with time-to-event data}
Randomized, double-blind, RCT. Patients were randomized equally to: (experimental drug + standard care) OR (standard care only).\\
Primary outcome of interest (or called ``endpoint'') is the progression-free survival (PFS), which is the time when the disease or cancer do not get worse. The endpoint was assessed with a \emph{log-rank test} and \emph{Cox regression} with treatment as a covariate.
\begin{itemize}
\item \emph{log-rank test}: compare the survival distributions of two or more groups. It tests the hypothesis that there is no difference in survival (or time-to-event) between the two groups. If the log-rank test indicates a significant difference, it suggests the treatment affects how long patients live without their disease worsening.
\item \emph{Cox regression}: estimate the hazard ratio between two groups, which tells us the relative risk of disease progression in the treatment group compared to the control group. If the $\text{HR} < 1$, it suggests that the new treatment delays disease progression better than the control treatment.
\end{itemize}
As for the DV, HR=0.7 was deemed necessary to be clinically meaningful. Values larger than 0.7 are unsatisfactory to clearly justify further development of the drug.\\
So the dual-criterion is:
\begin{enumerate}
\item Statistical significance: one-sided p-value of log-rank test $\leq 0.1$.
\item Clinical relevance: estimated HR from Cox regression $\leq 0.70$.
\end{enumerate}
\subsection{Reproduce Table 3}
Here we attempt to reproduce the values in Table 3. \\
For the first sub-table, similar to Section \ref{1.4}, when $n > n_{\text{min}}$, the cutting value $\hat{\theta}$ (\texttt{cut.ssig}) of the ``NO-GO'' decision is given by $\texttt{cut.ssig} = \log(1) - z_{\alpha}\sqrt{\frac{\sigma^2}{n}}$. These correspond to $\hat{\theta} > 0.736 \neq 0.7$ from the paper. \\
For the second sub-table, $n = n_{\text{min}}$.\\
For the third sub-table, it requires one-sided type I error of 0.1 and power of 0.9 for HR=0.5. So the cutting value for clinical relevance is chosen to satisfy these requirements. One thing to be answered is why the authors used 0.901 as the power for HR=0.5, rather than 0.9.\\
For the forth sub-table, it requires one-sided type I error of 0.1 and power of 0.8 for HR=0.5. The sample size $n=38 < n_{\text{min}}$. The cutting value of the decisions are calculated in a similar way as in Section \ref{1.4}. One thing to be answered here is that the current calculation returns the same values in the table, but the $\hat{\theta}$ is slightly different from 0.659 as claimed in the paper. Also, the power at HR=0.5 is 0.804, rather than 0.8. \\
For sub-table five, it used a type I error of 0.2 and a power of 0.9, with sample size $n=38 < n_{\text{min}}$. The cutting value of the decisions are calculated in a similar way as in Section \ref{1.4}. However the question to be answered is the same with that in sub-table 4.
<< Table 3, echo = TRUE, results=tex >>=
# minimum sample size
n.min <- ceiling((4*qnorm(0.9)^2)/(log(1)-log(0.7))^2) # n.min = 52

# a sequence of true log(HR).
t.d <- log(seq(0.5, 1, 0.1))

# Dual-criterion design: alpha=0.1, DV=log(0.7), n=70
n1 <- 70
sd1 <- sqrt((2^2)/n1) # standard deviation
cut.ssig <- log(1)-qnorm(0.9)* sqrt(2^2 / n1) # statistical significance
cut.crel <- log(0.7) # critical relevance
pp.go1 <- pnorm(cut.crel, t.d, sd1)
pp.ngo1 <- 1- pnorm(cut.ssig, t.d, sd1)
pp.intd1 <- 1 -pp.go1 - pp.ngo1

subtable1 <- matrix(data=round(c(exp(t.d), pp.go1,pp.ngo1,pp.intd1),3), ncol=4)

# Dual-criterion design: alpha=0.1, DV=0.7, n=52
n2 <- 52
sd2 <- sqrt((2^2)/n2)
cut.ssig <- log(0.7)
cut.crel <- log(0.7)
pp.go2 <- pnorm(cut.crel, t.d, sd2)
pp.ngo2 <- 1-pnorm(cut.ssig, t.d, sd2)
pp.intd2 <- 1 -pp.go2 - pp.ngo2

subtable2 <- matrix(data=round(c(exp(t.d), pp.go2,pp.ngo2,pp.intd2),3), ncol=4)

# Dual-criterion design: alpha=0.1, beta=0.1, n=55
n3 <- 55
sd3 <- sqrt((2^2)/n3)
cut.ssig <- qnorm(0.901,log(0.5),sd3) # reason for using 0.901 rather than 0.9?
cut.crel <- qnorm(0.901,log(0.5),sd3)
pp.go3 <- pnorm(cut.crel, t.d, sd3)
pp.ngo3 <- 1-pnorm(cut.ssig, t.d, sd3)
pp.intd3 <- 1 -pp.go3 - pp.ngo3

subtable3 <- matrix(data=round(c(exp(t.d), pp.go3,pp.ngo3,pp.intd3),3), ncol=4)

# Dual-criterion design: alpha=0.1, beta=0.2, n=38
n4 <- 38
sd4 <- sqrt((2^2)/n4)
cut.ssig <- log(1)-qnorm(0.9)* sqrt(2^2 / n4) # different from 0.659
cut.crel <- log(1)-qnorm(0.9)* sqrt(2^2 / n4)
pp.go4 <- pnorm(cut.crel, t.d, sd4)
pp.ngo4 <- 1-pnorm(cut.ssig, t.d, sd4)
pp.intd4 <- 1 -pp.go4 - pp.ngo4

subtable4 <- matrix(data=round(c(exp(t.d), pp.go4,pp.ngo4,pp.intd4),3), ncol=4)

# Dual-criterion design: alpha=0.2, beta=0.1, n=38
n5 <- 38
sd5 <- sqrt((2^2)/n5)
cut.ssig <- log(1)-qnorm(0.8)* sqrt(2^2 / n5)
# How is the power guaranteed to be 0.1? This is only using alpha=0.2.
cut.crel <- log(1)-qnorm(0.8)* sqrt(2^2 / n5)
pp.go5 <- pnorm(cut.crel, t.d, sd5)
pp.ngo5 <- 1-pnorm(cut.ssig, t.d, sd5)
pp.intd5 <- 1 -pp.go5 - pp.ngo5

subtable5 <- matrix(data=round(c(exp(t.d), pp.go5,pp.ngo5,pp.intd5),3), ncol=4)

# Combine subtables by rows
combined_table <- rbind(subtable1, subtable2, subtable3, subtable4, subtable5)

# Convert to data frame for better kable support
combined_table <- as.data.frame(combined_table)

kable(combined_table, format = "latex", align = "c", booktabs = TRUE,
      col.names = c("True HR", "GO", "NO-GO", "Inconclusive")) %>%
  kable_styling(full_width = FALSE, position = "center",
                latex_options = c("hold_position","scale_down")) %>%
  add_header_above(c("Reproduced Table 3" = 4)) %>%
  group_rows("Subtable 1", 1, 6) %>%
  group_rows("Subtable 2", 7, 12) %>%
  group_rows("Subtable 3", 13, 18) %>%
  group_rows("Subtable 4", 19, 24) %>%
  group_rows("Subtable 5", 25, 30)
@

\pagebreak
\section{Example 2: A single-arm PoC design with binary data}
Experimental drug in Chinese patients with non-small-cell lung cancer. \\
Primary endpoint is objective response rate (ORR), which quantifies the preliminary efficacy of the experimental drug. \\
Prior: minimally informative unimodal beta prior distribution $Beta(0.0811,1)$, which has mean 0.75.\\
NV is set to 7.5\% rather than 0, because of the absence of a comparator (in single arm trials). \\
DV is set to be 10\%+7.5\%=17.5\%.\\
So the dual-criterion is:
\begin{enumerate}
\item Bayesian statistical significance: $\mathbb{P}(ORR \geq 7.5\% | data) \geq 0.95$
\item Clinical relevance: Posterior median $\geq 17.5\%$
\end{enumerate}
The minimal sample size was 22. Final sample size 25. \\
Null hypothesis: there is no effect of the drug, i.e. ORR=7.5\%\\
$\mathbb{P}(\text{type I error})=\mathbb{P}(\text{reject } H_0 | H_0 \text{ is true})=\mathbb{P}(\text{reject } H_0 | ORR \leq 7.5\%)$\\
$\mathbb{P}(\text{type II error})=\mathbb{P}(\text{not reject } H_0 | H_0 \text{ is false})=\mathbb{P}(\text{reject } H_0 | ORR= \text{response rate})$\\
Table 4 results show that this dual-criterion design is a three-outcome design with desirable properties. 

\subsection{Reproduce Figure 2}
<<Figure 2, echo=FALSE>>=

@

\subsection{Reproduce Table 4}
<<Table 4, echo=FALSE>>=


@


%% References
\bibliographystyle{apalike}
\bibliography{bibliography}

\newpage
\section*{Computational details}
<< "sessionInfo" >>=
cat(paste(Sys.time(), Sys.timezone(), "\n"))
sessionInfo()
@

\end{document}
